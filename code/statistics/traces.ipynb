{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELAPSE Traces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "rows = [\n",
    "    (\"Adult\", \"(gender, age, race)\"),\n",
    "    (\"KDD\", \"(gender, age, race)\"),\n",
    "    (\"DC\", \"(gender, age)\"),\n",
    "    (\"MobiAct\", \"(gender, age)\"),\n",
    "    (\"ARS\", \"(gender)\"),\n",
    "    (\"celeba\", \"(gender, age)\"),\n",
    "    (\"fairface\", \"(age, race)\"),\n",
    "    (\"audioMNIST\", \"(gender, age)\"),\n",
    "    (\"voxceleb\", \"(race)\"),\n",
    "]\n",
    "\n",
    "\n",
    "with open(\"../../traces/DatasetProperties.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Dataset\", \"SensitiveAttributes\"])  \n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# --- Datasets et modèles autorisés par groupe ---\n",
    "# 1) \"SVM\", \"MLP\", \"LR\" avec \"Adult\", \"KDD\", \"DC\", \"MobiAct\", \"ARS\"\n",
    "group1_datasets = [\"Adult\", \"KDD\", \"DC\", \"MobiAct\", \"ARS\"]\n",
    "group1_models = [\"SVM\", \"MLP\", \"LR\"]\n",
    "\n",
    "# 2) \"celeba\", \"fairface\" avec \"ResNet18\", \"VGG\"\n",
    "group2_datasets = [\"celeba\", \"fairface\"]\n",
    "group2_models = [\"ResNet18\", \"VGG\"]\n",
    "\n",
    "# 3) \"audioMnist\", \"voxceleb\" avec \"LSTM\", \"CNN\"\n",
    "group3_datasets = [\"audioMnist\", \"voxceleb\"]\n",
    "group3_models = [\"LSTM\", \"CNN\"]\n",
    "\n",
    "# Construction de la table Dataset -> Modèles autorisés\n",
    "dataset_to_models = {}\n",
    "for d in group1_datasets:\n",
    "    dataset_to_models[d] = list(group1_models)\n",
    "for d in group2_datasets:\n",
    "    dataset_to_models[d] = list(group2_models)\n",
    "for d in group3_datasets:\n",
    "    dataset_to_models[d] = list(group3_models)\n",
    "\n",
    "# >>> Spécificité demandée : MobiAct uniquement MLP <<<\n",
    "dataset_to_models[\"MobiAct\"] = [\"MLP\"]\n",
    "\n",
    "# Liste ordonnée finale des datasets (pour l'ordre d’itération)\n",
    "datasets = group1_datasets + group2_datasets + group3_datasets\n",
    "\n",
    "# --- Autres paramètres ---\n",
    "selection_methods = [\"Full\", \"Craig\", \"Glister\", \"GradMatch\", \"Random\"]\n",
    "selection_ratios_partial = [0.05, 0.1, 0.2, 0.3]  # pour toutes les méthodes sauf \"Full\"\n",
    "selection_frequency = 20\n",
    "num_runs = 5\n",
    "\n",
    "# --- Génération des configurations ---\n",
    "configurations = []\n",
    "ec_id = 1\n",
    "\n",
    "for dataset in datasets:\n",
    "    models = dataset_to_models[dataset]\n",
    "    for model in models:\n",
    "        for method in selection_methods:\n",
    "            ratios = [1.0] if method == \"Full\" else selection_ratios_partial\n",
    "            for ratio in ratios:\n",
    "                configurations.append([\n",
    "                    ec_id,\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    method,\n",
    "                    ratio,\n",
    "                    selection_frequency,\n",
    "                    num_runs\n",
    "                ])\n",
    "                ec_id += 1\n",
    "\n",
    "# --- Écriture du CSV ---\n",
    "with open(\"../../traces/ExperimentConfigurations.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\n",
    "        \"EC_ID\",\n",
    "        \"dataset\",\n",
    "        \"model\",\n",
    "        \"system\",\n",
    "        \"ratio\",\n",
    "        \"Selection frequency\",\n",
    "        \"#Runs\"\n",
    "    ])\n",
    "    writer.writerows(configurations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/ars-selection/CRAIGPB',\n",
    "    '../../results/ars-selection/GLISTERPB',\n",
    "    '../../results/ars-selection/GradMatchPB',\n",
    "    '../../results/ars-selection/Random'\n",
    "]\n",
    "ratio_path = ['/ars_0.05', '/ars_0.1', '/ars_0.2', '/ars_0.3']\n",
    "directory_full = '../../results/ars-selection/Full/ars_1'\n",
    "models = ['Logreg', 'MLP', 'SVM']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'Logreg': (0, 150),\n",
    "        'MLP':    (0, 150),\n",
    "        'SVM':    (0, 80),\n",
    "    },\n",
    "    'full': {\n",
    "        'Logreg': (0, 150),\n",
    "        'MLP':    (0, 150),\n",
    "        'SVM':    (0, 80),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'ars'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'ars'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/ars_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/adult-selection/CRAIGPB',\n",
    "    '../../results/adult-selection/GLISTERPB',\n",
    "    '../../results/adult-selection/GradMatchPB',\n",
    "    '../../results/adult-selection/Random'\n",
    "]\n",
    "ratio_path = ['/adult_0.05', '/adult_0.1', '/adult_0.2', '/adult_0.3']\n",
    "directory_full = '../../results/adult-selection/Full/adult_1'\n",
    "models = ['Logreg', 'MLP', 'SVM']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'Logreg': (0, 150),\n",
    "        'MLP':    (0, 400),\n",
    "        'SVM':    (0, 150),\n",
    "    },\n",
    "    'full': {\n",
    "        'Logreg': (0, 150),\n",
    "        'MLP':    (0, 400),\n",
    "        'SVM':    (0, 150),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'adult'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'adult'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/adult_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/kdd-selection/CRAIGPB',\n",
    "    '../../results/kdd-selection/GLISTERPB',\n",
    "    '../../results/kdd-selection/GradMatchPB',\n",
    "    '../../results/kdd-selection/Random'\n",
    "]\n",
    "ratio_path = ['/kdd_0.05', '/kdd_0.1', '/kdd_0.2', '/kdd_0.3']\n",
    "directory_full = '../../results/kdd-selection/Full/kdd_1'\n",
    "models = ['Logreg', 'MLP', 'SVM']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'Logreg': (0, 80),\n",
    "        'MLP':    (0, 80),\n",
    "        'SVM':    (0, 80),\n",
    "    },\n",
    "    'full': {\n",
    "        'Logreg': (0, 80),\n",
    "        'MLP':    (0, 80),\n",
    "        'SVM':    (0, 80),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'kdd'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'kdd'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/kdd_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/dc-selection/CRAIGPB',\n",
    "    '../../results/dc-selection/GLISTERPB',\n",
    "    '../../results/dc-selection/GradMatchPB',\n",
    "    '../../results/dc-selection/Random'\n",
    "]\n",
    "ratio_path = ['/dc_0.05', '/dc_0.1', '/dc_0.2', '/dc_0.3']\n",
    "directory_full = '../../results/dc-selection/Full/dc_1'\n",
    "models = ['Logreg', 'MLP', 'SVM']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'Logreg': (0, 280),\n",
    "        'MLP':    (0, 120),\n",
    "        'SVM':    (0, 120),\n",
    "    },\n",
    "    'full': {\n",
    "        'Logreg': (0, 280),\n",
    "        'MLP':    (0, 120),\n",
    "        'SVM':    (0, 120),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'dc'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'dc'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/dc_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace écrite: ../../results/test/mobiact_epoch_traces.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "BASE_DIR = '../../results/mobiact-selection'  # <- sans le doublon 'results'\n",
    "systems = ['CRAIGPB', 'GLISTERPB', 'GradMatchPB', 'Random']\n",
    "systems_path = [os.path.join(BASE_DIR, s) for s in systems]\n",
    "ratio_path = ['/mobiact_0.05', '/mobiact_0.1', '/mobiact_0.2', '/mobiact_0.3']  # ajoute /mobiact_0.5 si tu en as\n",
    "directory_full = os.path.join(BASE_DIR, 'Full', 'mobiact_1')\n",
    "models = ['MLP']\n",
    "excluded_columns = []\n",
    "\n",
    "# Fenêtres iloc spécifiques\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {'MLP': (0, 300)},\n",
    "    'full': {'MLP': (0, 300)},\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "# --- Tolérance sur les noms de fichiers ---\n",
    "FAIR_PATTERNS = [\n",
    "    'train_mobiact_fair_metrics_',  # ex: train_mobiact_fair_metrics_..._12.csv\n",
    "    'mobiact_fair_metrics_',\n",
    "    'fair_metrics_',                # ex: fair_metrics_MLP_12.csv\n",
    "]\n",
    "COST_PATTERNS = [\n",
    "    'train_mobiact_cost_metrics_',\n",
    "    'mobiact_cost_metrics_',\n",
    "    'cost_metrics_',\n",
    "]\n",
    "\n",
    "RUNID_REGEXES = [\n",
    "    re.compile(r'_(\\d+)\\.csv$'),     # ..._12.csv\n",
    "    re.compile(r'run[_-]?(\\d+)\\.csv$'), # ...run12.csv ou ...run_12.csv\n",
    "    re.compile(r'-(\\d+)\\.csv$'),     # ...-12.csv\n",
    "]\n",
    "\n",
    "def extract_run_id(filename: str):\n",
    "    for rgx in RUNID_REGEXES:\n",
    "        m = rgx.search(filename)\n",
    "        if m:\n",
    "            try:\n",
    "                return int(m.group(1))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def match_cost_for_fair(dir_path: str, fair_filename: str, run_id: int | None):\n",
    "    \"\"\"\n",
    "    Cherche le cost correspondant :\n",
    "    1) même run_id si possible\n",
    "    2) nom voisin en remplaçant 'fair'->'cost'\n",
    "    3) s'il n'y a qu'un seul cost dans le dossier, on le prend\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(dir_path) if f.endswith('.csv')]\n",
    "    cost_files = [f for f in files if any(f.startswith(p) for p in COST_PATTERNS)]\n",
    "\n",
    "    # 1) par run_id\n",
    "    if run_id is not None:\n",
    "        for cf in cost_files:\n",
    "            if extract_run_id(cf) == run_id:\n",
    "                return os.path.join(dir_path, cf)\n",
    "\n",
    "    # 2) par nom voisin\n",
    "    candidates = []\n",
    "    neighbor = fair_filename.replace('fair', 'cost')\n",
    "    for cf in cost_files:\n",
    "        if cf == neighbor:\n",
    "            return os.path.join(dir_path, cf)\n",
    "        # tolérance: même tronc + cost\n",
    "        if cf.split('.csv')[0].endswith(str(run_id)) and run_id is not None:\n",
    "            candidates.append(cf)\n",
    "\n",
    "    if candidates:\n",
    "        return os.path.join(dir_path, candidates[0])\n",
    "\n",
    "    # 3) unique cost dans le dossier\n",
    "    if len(cost_files) == 1:\n",
    "        return os.path.join(dir_path, cost_files[0])\n",
    "\n",
    "    return None\n",
    "\n",
    "def load_epochs_from_fair(fair_path: str, model: str, phase: str):\n",
    "    start, stop = _slice_window(model, phase)\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie 'Accuracy' dans le fair\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "    return fair_df\n",
    "\n",
    "def load_first_training_time(cost_path: str):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def list_matching_fair_files(dir_path: str):\n",
    "    return [\n",
    "        f for f in os.listdir(dir_path)\n",
    "        if f.endswith('.csv') and any(f.startswith(p) for p in FAIR_PATTERNS)\n",
    "    ]\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "if os.path.isdir(directory_full):\n",
    "    for fair_name in list_matching_fair_files(directory_full):\n",
    "        fair_file = os.path.join(directory_full, fair_name)\n",
    "        run_id = extract_run_id(fair_name)\n",
    "        cost_file = match_cost_for_fair(directory_full, fair_name, run_id)\n",
    "        if not cost_file:\n",
    "            continue\n",
    "\n",
    "        df_metrics = load_epochs_from_fair(fair_file, 'MLP', 'full')\n",
    "        full_time = load_first_training_time(cost_file)\n",
    "\n",
    "        df_metrics['dataset'] = 'mobiact'\n",
    "        df_metrics['model'] = 'MLP'\n",
    "        df_metrics['system'] = 'Full'\n",
    "        df_metrics['ratio'] = 1.0\n",
    "        df_metrics['Full_training_time'] = full_time\n",
    "        df_metrics['runID'] = run_id if run_id is not None else -1\n",
    "\n",
    "        all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for sys_dir in systems_path:\n",
    "        if not os.path.isdir(sys_dir):\n",
    "            continue\n",
    "        system = os.path.basename(sys_dir)\n",
    "        for r in ratio_path:\n",
    "            directory = sys_dir + r\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "            ratio = float(r.split('_')[-1])\n",
    "\n",
    "            for fair_name in list_matching_fair_files(directory):\n",
    "                fair_file = os.path.join(directory, fair_name)\n",
    "                run_id = extract_run_id(fair_name)\n",
    "                cost_file = match_cost_for_fair(directory, fair_name, run_id)\n",
    "                if not cost_file:\n",
    "                    continue\n",
    "\n",
    "                df_metrics = load_epochs_from_fair(fair_file, model, 'selection')\n",
    "                full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                df_metrics['dataset'] = 'mobiact'\n",
    "                df_metrics['model'] = model\n",
    "                df_metrics['system'] = system\n",
    "                df_metrics['ratio'] = ratio\n",
    "                df_metrics['Full_training_time'] = full_time\n",
    "                df_metrics['runID'] = run_id if run_id is not None else -1\n",
    "\n",
    "                all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    # aide au debug: affiche quelques fichiers trouvés dans Full + un dossier de sélection\n",
    "    dbg = []\n",
    "    if os.path.isdir(directory_full):\n",
    "        dbg += [os.path.join(directory_full, f) for f in os.listdir(directory_full)[:10]]\n",
    "    for p in systems_path:\n",
    "        d = p + ratio_path[0]\n",
    "        if os.path.isdir(d):\n",
    "            dbg += [os.path.join(d, f) for f in os.listdir(d)[:10]]\n",
    "            break\n",
    "    raise ValueError(\n",
    "        \"Aucun fichier trouvé : vérifie les NOMS de fichiers fair/cost.\\n\"\n",
    "        f\"Patrons fair acceptés: {FAIR_PATTERNS}\\n\"\n",
    "        f\"Patrons cost acceptés: {COST_PATTERNS}\\n\"\n",
    "        f\"Exemples de fichiers vus: {dbg}\"\n",
    "    )\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "out_dir = '../../results/test'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_csv = os.path.join(out_dir, 'mobiact_epoch_traces.csv')\n",
    "df_all_epochs.to_csv(out_csv, index=False)\n",
    "print(f\"Trace écrite: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/celeba-selection/CRAIGPB',\n",
    "    '../../results/celeba-selection/GLISTERPB',\n",
    "    '../../results/celeba-selection/GradMatchPB',\n",
    "    '../../results/celeba-selection/Random'\n",
    "]\n",
    "ratio_path = ['/celeba_0.05', '/celeba_0.1', '/celeba_0.2', '/celeba_0.3']\n",
    "directory_full = '../../results/celeba-selection/Full/celeba_1'\n",
    "models = ['ResNet18', 'VGG']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'ResNet18': (0, 150),\n",
    "        'VGG':    (0, 150),\n",
    "    },\n",
    "    'full': {\n",
    "        'ResNet18': (0, 150),\n",
    "        'VGG':    (0, 150),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'celeba'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'celeba'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/celeba_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/fairface-selection/CRAIGPB',\n",
    "    '../../results/fairface-selection/GLISTERPB',\n",
    "    '../../results/fairface-selection/GradMatchPB',\n",
    "    '../../results/fairface-selection/Random'\n",
    "]\n",
    "ratio_path = ['/fairface_0.05', '/fairface_0.1', '/fairface_0.2', '/fairface_0.3']\n",
    "directory_full = '../../results/fairface-selection/Full/fairface_1'\n",
    "models = ['ResNet18', 'VGG']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'ResNet18': (0, 150),\n",
    "        'VGG':    (0, 150),\n",
    "    },\n",
    "    'full': {\n",
    "        'ResNet18': (0, 150),\n",
    "        'VGG':    (0, 150),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'fairface'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'fairface'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/fairface_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/audiomnist-selection/CRAIGPB',\n",
    "    '../../results/audiomnist-selection/GLISTERPB',\n",
    "    '../../results/audiomnist-selection/GradMatchPB',\n",
    "    '../../results/audiomnist-selection/Random'\n",
    "]\n",
    "ratio_path = ['/audiomnist_0.05', '/audiomnist_0.1', '/audiomnist_0.2', '/audiomnist_0.3']\n",
    "directory_full = '../../results/audiomnist-selection/Full/audiomnist_1'\n",
    "models = ['AudioCNN', 'AudioLSTM']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'AudioCNN': (0, 150),\n",
    "        'AudioLSTM':    (0, 150),\n",
    "    },\n",
    "    'full': {\n",
    "        'AudioCNN': (0, 150),\n",
    "        'AudioLSTM':    (0, 150),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'audiomnist'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'audiomnist'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/audiomnist_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths & settings ===\n",
    "systems_path = [\n",
    "    '../../results/voxceleb-selection/CRAIGPB',\n",
    "    '../../results/voxceleb-selection/GLISTERPB',\n",
    "    '../../results/voxceleb-selection/GradMatchPB',\n",
    "    '../../results/voxceleb-selection/Random'\n",
    "]\n",
    "ratio_path = ['/voxceleb_0.05', '/voxceleb_0.1', '/voxceleb_0.2', '/voxceleb_0.3']\n",
    "directory_full = '../../results/voxceleb-selection/Full/voxceleb_1'\n",
    "models = ['AudioCNN', 'AudioLSTM']\n",
    "excluded_columns = []  # add any columns to drop from fair metrics\n",
    "\n",
    "\n",
    "# --- Model-specific row windows (start inclusive, stop exclusive) ---\n",
    "MODEL_WINDOWS = {\n",
    "    'selection': {\n",
    "        'AudioCNN': (0, 150),\n",
    "        'AudioLSTM':    (0, 150),\n",
    "    },\n",
    "    'full': {\n",
    "        'AudioCNN': (0, 150),\n",
    "        'AudioLSTM':    (0, 150),\n",
    "    }\n",
    "}\n",
    "\n",
    "def _slice_window(model: str, phase: str):\n",
    "    try:\n",
    "        return MODEL_WINDOWS[phase][model]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Aucune fenêtre définie pour phase='{phase}', model='{model}'\")\n",
    "\n",
    "def _clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop unnamed index-like columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "    return df\n",
    "\n",
    "def _ensure_epoch_id_inplace(df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renomme 'epoch' -> 'epochID' (ou 'Epoch' -> 'epochID').\n",
    "    Si aucune des deux colonnes n'existe, crée 'epochID' = range(len(df_out)).\n",
    "    Aucune colonne 'epoch'/'Epoch' ne reste après appel.\n",
    "    \"\"\"\n",
    "    if 'epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'epoch': 'epochID'}, inplace=True)\n",
    "    elif 'Epoch' in df_out.columns:\n",
    "        df_out.rename(columns={'Epoch': 'epochID'}, inplace=True)\n",
    "    else:\n",
    "        df_out['epochID'] = list(range(len(df_out)))\n",
    "\n",
    "def load_epochs(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'selection')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    # Retirer colonnes exclues, ne pas multiplier par 100, ne pas toucher Accuracy\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Vérifie présence d'Accuracy (vient uniquement de fair_path)\n",
    "    _ = fair_df['Accuracy']  # lève KeyError si absent\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df  # fair_df contient déjà epochID\n",
    "\n",
    "def load_epochs_full(fair_path, model):\n",
    "    start, stop = _slice_window(model, 'full')\n",
    "    fair_df_raw = pd.read_csv(fair_path)\n",
    "    fair_df_raw = _clean_df(fair_df_raw)\n",
    "\n",
    "    fair_df = fair_df_raw.iloc[start:stop].copy()\n",
    "    fair_df = fair_df.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    _ = fair_df['Accuracy']  # assert presence\n",
    "\n",
    "    # Assure epochID (rename en place, sans doublon)\n",
    "    _ensure_epoch_id_inplace(fair_df)\n",
    "\n",
    "    return fair_df_raw, fair_df\n",
    "\n",
    "def load_first_training_time(cost_path):\n",
    "    df = pd.read_csv(cost_path)\n",
    "    df = _clean_df(df)\n",
    "    return df['Full_training_time'].iloc[0] if 'Full_training_time' in df.columns and len(df) > 0 else None\n",
    "\n",
    "def extract_run_id(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_epochs = []\n",
    "\n",
    "# === Full traces ===\n",
    "for model in models:\n",
    "    if not os.path.isdir(directory_full):\n",
    "        continue\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "            run_id = extract_run_id(filename)\n",
    "            if run_id is None:\n",
    "                continue\n",
    "\n",
    "            fair_file = os.path.join(directory_full, filename)\n",
    "\n",
    "            cost_file = None\n",
    "            for f in os.listdir(directory_full):\n",
    "                if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                    cost_file = os.path.join(directory_full, f)\n",
    "                    break\n",
    "            if not cost_file:\n",
    "                continue\n",
    "\n",
    "            _, df_metrics = load_epochs_full(fair_file, model)\n",
    "            full_time = load_first_training_time(cost_file)\n",
    "\n",
    "            df_metrics['dataset'] = 'voxceleb'\n",
    "            df_metrics['model'] = model\n",
    "            df_metrics['system'] = 'Full'\n",
    "            df_metrics['ratio'] = 1.0\n",
    "            df_metrics['Full_training_time'] = full_time\n",
    "            df_metrics['runID'] = run_id\n",
    "\n",
    "            all_epochs.append(df_metrics)\n",
    "\n",
    "# === Selection traces ===\n",
    "for model in models:\n",
    "    for directory_path_1 in systems_path:\n",
    "        for directory_path_2 in ratio_path:\n",
    "            directory = directory_path_1 + directory_path_2\n",
    "            if not os.path.isdir(directory):\n",
    "                continue\n",
    "\n",
    "            system = directory_path_1.split('/')[-1]\n",
    "            ratio = float(directory_path_2.split('_')[-1])\n",
    "\n",
    "            for filename in os.listdir(directory):\n",
    "                if f\"fair_metrics_{model}_\" in filename and filename.endswith('.csv'):\n",
    "                    run_id = extract_run_id(filename)\n",
    "                    if run_id is None:\n",
    "                        continue\n",
    "\n",
    "                    fair_file = os.path.join(directory, filename)\n",
    "\n",
    "                    cost_file = None\n",
    "                    for f in os.listdir(directory):\n",
    "                        if f.startswith(f\"cost_metrics_{model}_\") and f.endswith(f\"{run_id}.csv\"):\n",
    "                            cost_file = os.path.join(directory, f)\n",
    "                            break\n",
    "                    if not cost_file:\n",
    "                        continue\n",
    "\n",
    "                    _, df_metrics = load_epochs(fair_file, model)\n",
    "                    full_time = load_first_training_time(cost_file)\n",
    "\n",
    "                    df_metrics['dataset'] = 'voxceleb'\n",
    "                    df_metrics['model'] = model\n",
    "                    df_metrics['system'] = system\n",
    "                    df_metrics['ratio'] = ratio\n",
    "                    df_metrics['Full_training_time'] = full_time\n",
    "                    df_metrics['runID'] = run_id\n",
    "\n",
    "                    all_epochs.append(df_metrics)\n",
    "\n",
    "if not all_epochs:\n",
    "    raise ValueError(\"Aucun fichier trouvé : vérifie les noms des fichiers fair_metrics et cost_metrics.\")\n",
    "\n",
    "df_all_epochs = pd.concat(all_epochs, ignore_index=True)\n",
    "\n",
    "os.makedirs('../../results/test', exist_ok=True)\n",
    "df_all_epochs.to_csv('../../results/test/voxceleb_epoch_traces.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_csv_files(file_paths):\n",
    "    target_columns = [\n",
    "       'SPD_gender', 'EOD_gender', 'AOD_gender', 'DI_gender', 'DcI_gender',\n",
    "       'SPD_race', 'EOD_race', 'AOD_race', 'DI_race', 'DcI_race', 'SPD_age',\n",
    "       'EOD_age', 'AOD_age', 'DI_age', 'DcI_age', 'F1_score', 'Precision',\n",
    "       'Recall', 'Accuracy', 'dataset', 'model', 'system', 'ratio',\n",
    "       'Full_training_time', 'runID', 'epochID'\n",
    "    ]\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path, usecols=lambda c: c in target_columns, engine='python')\n",
    "        missing_cols = set(target_columns) - set(df.columns)\n",
    "        for col in missing_cols:\n",
    "            df[col] = pd.NA\n",
    "        df = df.reindex(columns=target_columns)\n",
    "        dfs.append(df)\n",
    "\n",
    "    result = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283900\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['../../results/test/ars_epoch_traces.csv', \n",
    "              '../../results/test/dc_epoch_traces.csv', \n",
    "              '../../results/test//mobiact_epoch_traces.csv', \n",
    "              '../../results/test/adult_epoch_traces.csv', \n",
    "              '../../results/test/kdd_epoch_traces.csv',\n",
    "              '../../results/test/celeba_epoch_traces.csv', \n",
    "              '../../results/test/fairface_epoch_traces.csv',\n",
    "              '../../results/test/audiomnist_epoch_traces.csv',\n",
    "              '../../results/test/voxceleb_epoch_traces.csv' \n",
    "              ]\n",
    "result_df = concatenate_csv_files(file_paths)\n",
    "result_df.to_csv('../../results/test/epoch_traces.csv', index=False)\n",
    "print(len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] écrit : ../../results/test/epoch_traces_EC_ID.csv  (283900 lignes)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RES_PATH = Path(\"../../results/test/epoch_traces.csv\")\n",
    "CFG_PATH = Path(\"../../traces/ExperimentConfigurations.csv\")\n",
    "OUT_PATH = Path(\"../../results/test/epoch_traces_EC_ID.csv\")\n",
    "\n",
    "# --- normalisations cohérentes ---\n",
    "def norm_str(s):\n",
    "    return str(s).strip()\n",
    "\n",
    "def to_key(s):\n",
    "    return norm_str(s).lower()\n",
    "\n",
    "# Dictionnaires en MINUSCULES car on utilise .lower() pour la clé\n",
    "MAP_MODEL = {\n",
    "    'logreg':'LR','lr':'LR',\n",
    "    'mlp':'MLP',\n",
    "    'svm':'SVM',\n",
    "    'audiolstm':'LSTM','lstm':'LSTM',\n",
    "    'audiocnn':'CNN','cnn':'CNN',\n",
    "    'dc':'DC'\n",
    "}\n",
    "MAP_DATASET = {\n",
    "    'census':'Adult','adult':'Adult',\n",
    "    'kdd':'KDD',\n",
    "    'dc':'DC',\n",
    "    'mobiact':'MobiAct',\n",
    "    'ars':'ARS',\n",
    "    'celeba':'CelebA',\n",
    "    'fairface':'Fairface',\n",
    "    'audiomnist':'audioMNIST',\n",
    "    'voxceleb':'voxceleb'  # la config que tu génères utilise 'voxceleb' en minuscules\n",
    "}\n",
    "# IMPORTANT: retirer les suffixes \"PB\" et normaliser le nom du système\n",
    "MAP_SYSTEM = {\n",
    "    'full':'Full',\n",
    "    'craig':'Craig','craigpb':'Craig',\n",
    "    'glister':'Glister','glisterpb':'Glister',\n",
    "    'gradmatch':'GradMatch','gradmatchpb':'GradMatch',\n",
    "    'random':'Random'\n",
    "}\n",
    "\n",
    "def canon_model(v):\n",
    "    k = to_key(v)\n",
    "    return MAP_MODEL.get(k, norm_str(v))\n",
    "\n",
    "def canon_dataset(v):\n",
    "    k = to_key(v)\n",
    "    return MAP_DATASET.get(k, norm_str(v))\n",
    "\n",
    "def canon_system(v):\n",
    "    k = to_key(v)\n",
    "    # retire un éventuel suffixe '-pb' ou 'pb' isolé\n",
    "    k = k.replace('-pb','').replace('_pb','')\n",
    "    if k.endswith('pb'):\n",
    "        k = k[:-2]\n",
    "    return MAP_SYSTEM.get(k, norm_str(v))\n",
    "\n",
    "def canon_ratio(x, ndigits=3):\n",
    "    try:\n",
    "        return round(float(x), ndigits)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- lecture ---\n",
    "df_res = pd.read_csv(RES_PATH, dtype=str)\n",
    "df_cfg = pd.read_csv(CFG_PATH, dtype=str)\n",
    "\n",
    "# hygiène colonnes\n",
    "need_res = {\"dataset\",\"model\",\"system\",\"ratio\"}\n",
    "if not need_res.issubset(df_res.columns):\n",
    "    missing = need_res - set(df_res.columns)\n",
    "    raise ValueError(f\"[epoch_traces.csv] colonnes manquantes: {missing}\")\n",
    "\n",
    "need_cfg = {\"EC_ID\",\"dataset\",\"model\",\"system\",\"ratio\"}\n",
    "if not need_cfg.issubset(df_cfg.columns):\n",
    "    missing = need_cfg - set(df_cfg.columns)\n",
    "    raise ValueError(f\"[ExperimentConfigurations.csv] colonnes manquantes: {missing}\")\n",
    "\n",
    "# --- canonisation des deux côtés ---\n",
    "# résultats\n",
    "df_res[\"ds_c\"]     = df_res[\"dataset\"].map(canon_dataset)\n",
    "df_res[\"model_c\"]  = df_res[\"model\"].map(canon_model)\n",
    "df_res[\"system_c\"] = df_res[\"system\"].map(canon_system)\n",
    "df_res[\"ratio_c\"]  = df_res[\"ratio\"].map(lambda x: canon_ratio(x, ndigits=3))\n",
    "\n",
    "# config\n",
    "df_cfg[\"ds_c\"]     = df_cfg[\"dataset\"].map(canon_dataset)\n",
    "df_cfg[\"model_c\"]  = df_cfg[\"model\"].map(canon_model)\n",
    "df_cfg[\"system_c\"] = df_cfg[\"system\"].map(canon_system)\n",
    "df_cfg[\"ratio_c\"]  = df_cfg[\"ratio\"].map(lambda x: canon_ratio(x, ndigits=3))\n",
    "\n",
    "# --- dédup config par clé canonique (évite duplication après merge) ---\n",
    "key = [\"ds_c\",\"model_c\",\"system_c\",\"ratio_c\"]\n",
    "dup = df_cfg.duplicated(key).sum()\n",
    "if dup:\n",
    "    print(f\"[WARN] {dup} doublon(s) dans la config sur la clé canonique. On garde le premier.\")\n",
    "df_cfg_u = df_cfg.drop_duplicates(key, keep=\"first\")\n",
    "\n",
    "# --- jointure m:1 sur la clé canonique ---\n",
    "df_merge = df_res.merge(df_cfg_u[key + [\"EC_ID\"]], on=key, how=\"left\", validate=\"m:1\")\n",
    "\n",
    "# --- diag des NaN restants ---\n",
    "nan_mask = df_merge[\"EC_ID\"].isna()\n",
    "nan_count = int(nan_mask.sum())\n",
    "if nan_count:\n",
    "    print(f\"[WARN] {nan_count} ligne(s) sans EC_ID après normalisation.\")\n",
    "    print(\"Top (dataset, model, system, ratio) manquants :\")\n",
    "    print(df_merge.loc[nan_mask, key].value_counts().head(15).to_string())\n",
    "\n",
    "# --- sortie : on reprend les colonnes d'origine + EC_ID propre ---\n",
    "df_out = df_res.copy()\n",
    "df_out[\"EC_ID\"] = df_merge[\"EC_ID\"]\n",
    "df_out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"[OK] écrit : {OUT_PATH}  ({len(df_out)} lignes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../../results/test/epoch_traces_EC_ID.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'runID': 'Run ID',\n",
    "    'epochID': 'Epoch ID',\n",
    "    'accuracy': 'Accuracy',\n",
    "    'Full_training_time': 'Time'\n",
    "})\n",
    "\n",
    "\n",
    "desired_order = [\n",
    "    'EC_ID', 'Run ID', 'Epoch ID', 'Time', 'Accuracy',\n",
    "    'F1_score', 'Precision', 'Recall',\n",
    "    'SPD_gender', 'EOD_gender', 'AOD_gender', 'DI_gender', 'DcI_gender',\n",
    "    'SPD_age', 'EOD_age', 'AOD_age', 'DI_age', 'DcI_age',\n",
    "    'SPD_race', 'EOD_race', 'AOD_race', 'DI_race', 'DcI_race'\n",
    "]\n",
    "\n",
    "columns_to_keep = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "df.to_csv('../../results/test/ExperimentMeasurements.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Écrit: ExperimentMeasurements_Adult_KDD.csv  (79900 lignes, 23 colonnes)\n",
      "[OK] Écrit: ExperimentMeasurements_DC_MobiAct_CelebA_audioMNIST.csv  (120700 lignes, 18 colonnes)\n",
      "[OK] Écrit: ExperimentMeasurements_ARS.csv  (32300 lignes, 13 colonnes)\n",
      "[OK] Écrit: ExperimentMeasurements_fairface.csv  (25500 lignes, 18 colonnes)\n",
      "[OK] Écrit: ExperimentMeasurements_voxceleb.csv  (25500 lignes, 13 colonnes)\n",
      "\n",
      "=== RÉCAP ===\n",
      "Lignes dans le fichier source                  : 283900\n",
      "Lignes couvertes par les 5 groupes             : 283900\n",
      "Somme des lignes dans les fichiers générés     : 283900\n",
      "[CHECK] Somme cohérente (somme = lignes couvertes).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- chemins ----------\n",
    "IN_PATH  = Path(\"../../results/test/epoch_traces_EC_ID.csv\")\n",
    "OUT_DIR  = Path(\"../../traces/\")\n",
    "\n",
    "# ---------- lecture ----------\n",
    "df = pd.read_csv(IN_PATH, dtype=str)\n",
    "initial_rows = len(df)\n",
    "\n",
    "# --- renommer les colonnes si elles existent (idempotent) ---\n",
    "rename_map = {\n",
    "    'runID': 'Run ID',\n",
    "    'epochID': 'Epoch ID',\n",
    "    'Full_training_time': 'Time',\n",
    "    'accuracy': 'Accuracy',\n",
    "}\n",
    "existing_map = {k: v for k, v in rename_map.items() if k in df.columns}\n",
    "if existing_map:\n",
    "    df = df.rename(columns=existing_map)\n",
    "\n",
    "# hygiène de base\n",
    "for col in [\"EC_ID\", \"dataset\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Colonne manquante dans {IN_PATH.name}: '{col}'\")\n",
    "df[\"EC_ID\"] = df[\"EC_ID\"].astype(str).str.strip()\n",
    "df[\"dataset\"] = df[\"dataset\"].astype(str).str.strip()\n",
    "\n",
    "# normalisation dataset (casse/espaces)\n",
    "norm = lambda s: str(s).strip().lower()\n",
    "df[\"dataset_norm\"] = df[\"dataset\"].map(norm)\n",
    "\n",
    "# ---------- colonnes demandées (avec l'ordre exact) ----------\n",
    "BASE   = ['EC_ID', 'Run ID', 'Epoch ID', 'Time', 'Accuracy', 'F1_score', 'Precision', 'Recall']\n",
    "GENDER = ['SPD_gender', 'EOD_gender', 'AOD_gender', 'DI_gender', 'DcI_gender']\n",
    "AGE    = ['SPD_age', 'EOD_age', 'AOD_age', 'DI_age', 'DcI_age']\n",
    "RACE   = ['SPD_race', 'EOD_race', 'AOD_race', 'DI_race', 'DcI_race']\n",
    "\n",
    "def n(x): return norm(x)\n",
    "\n",
    "groups = [\n",
    "    {\n",
    "        \"name\": \"Adult_KDD\",\n",
    "        \"datasets\": {n(\"Adult\"), n(\"KDD\")},\n",
    "        \"cols\": BASE + GENDER + AGE + RACE\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DC_MobiAct_CelebA_audioMNIST\",\n",
    "        \"datasets\": {n(\"DC\"), n(\"MobiAct\"), n(\"CelebA\"), n(\"audioMNIST\")},\n",
    "        \"cols\": BASE + GENDER + AGE\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ARS\",\n",
    "        \"datasets\": {n(\"ARS\")},\n",
    "        \"cols\": BASE + GENDER\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"fairface\",\n",
    "        \"datasets\": {n(\"fairface\")},\n",
    "        \"cols\": BASE + AGE + RACE\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"voxceleb\",\n",
    "        \"datasets\": {n(\"voxceleb\")},\n",
    "        \"cols\": BASE + RACE\n",
    "    },\n",
    "]\n",
    "\n",
    "# ---------- utilitaire d’écriture ----------\n",
    "def write_group(df_all: pd.DataFrame, datasets_norm: set, desired_cols: list, out_name: str) -> int:\n",
    "    sub = df_all[df_all[\"dataset_norm\"].isin(datasets_norm)].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[INFO] Aucun enregistrement pour {out_name} (datasets={sorted(datasets_norm)})\")\n",
    "        return 0\n",
    "\n",
    "    # enlever dataset/dataset_norm des exports\n",
    "    sub.drop(columns=[c for c in [\"dataset\", \"dataset_norm\"] if c in sub.columns],\n",
    "             inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # ne garder QUE les colonnes existantes parmi celles demandées (et donc dans l'ordre voulu)\n",
    "    present_cols = [c for c in desired_cols if c in sub.columns]\n",
    "    sub = sub[present_cols]\n",
    "\n",
    "    # tri si dispo\n",
    "    for k in [\"Run ID\", \"Epoch ID\"]:\n",
    "        if k in sub.columns:\n",
    "            sub[k] = pd.to_numeric(sub[k], errors=\"ignore\")\n",
    "    sort_keys = [k for k in [\"EC_ID\", \"Run ID\", \"Epoch ID\"] if k in sub.columns]\n",
    "    if sort_keys:\n",
    "        sub = sub.sort_values(sort_keys)\n",
    "\n",
    "    out_path = OUT_DIR / f\"ExperimentMeasurements_{out_name}.csv\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(f\"[OK] Écrit: {out_path.name}  ({len(sub)} lignes, {len(sub.columns)} colonnes)\")\n",
    "    return len(sub)\n",
    "\n",
    "# ---------- génération ----------\n",
    "total_rows = 0\n",
    "covered_mask = pd.Series(False, index=df.index)\n",
    "for g in groups:\n",
    "    total_rows += write_group(df, g[\"datasets\"], g[\"cols\"], g[\"name\"])\n",
    "    covered_mask = covered_mask | df[\"dataset_norm\"].isin(g[\"datasets\"])\n",
    "\n",
    "# ---------- check final ----------\n",
    "covered_rows = int(covered_mask.sum())\n",
    "uncovered_rows = initial_rows - covered_rows\n",
    "\n",
    "print(\"\\n=== RÉCAP ===\")\n",
    "print(f\"Lignes dans le fichier source                  : {initial_rows}\")\n",
    "print(f\"Lignes couvertes par les 5 groupes             : {covered_rows}\")\n",
    "print(f\"Somme des lignes dans les fichiers générés     : {total_rows}\")\n",
    "if uncovered_rows > 0:\n",
    "    missing_ds = (df.loc[~covered_mask, \"dataset\"].fillna(\"(NA)\").value_counts())\n",
    "    print(f\"[INFO] {uncovered_rows} ligne(s) hors groupes. Répartition :\")\n",
    "    print(missing_ds.to_string())\n",
    "\n",
    "if total_rows == covered_rows:\n",
    "    print(\"[CHECK] Somme cohérente (somme = lignes couvertes).\")\n",
    "else:\n",
    "    print(\"[CHECK] Incohérence : somme ≠ lignes couvertes. Vérifie la présence/orthographe des colonnes et datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_var = \"../../results/test_var/std_avg_vc_ttest_results_with_vc_4c.csv\"\n",
    "path_sel = \"../../results/test/ttest_5-5c-w-random.csv\"\n",
    "\n",
    "df_var = pd.read_csv(path_var)\n",
    "\n",
    "\n",
    "df_var = df_var.rename(columns={\n",
    "    col: col + \"_var\"\n",
    "    for col in df_var.columns if col.startswith(\"test_\")\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "df_sel = pd.read_csv(path_sel)\n",
    "\n",
    "join_cols = ['dataset', 'model', 'system', 'ratio']\n",
    "\n",
    "\n",
    "df_merged = None\n",
    "df_merged = pd.merge(df_sel, df_var, on=join_cols, how='inner')\n",
    "\n",
    "df_merged.to_csv(\"../../results/test/t-tests.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"../../results/test/t-tests.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "rename_map = {\n",
    "    'test_acc': 'test_Accuracy',\n",
    "    'test_acc_var': 'test_Accuracy_var',\n",
    "    'test_f1': 'test_F1_score',\n",
    "    'test_f1_var': 'test_F1_score_var',\n",
    "    'test_recall': 'test_Recall',\n",
    "    'test_recall_var': 'test_Recall_var',\n",
    "    'test_precision': 'test_Precision',\n",
    "    'test_precision_var': 'test_Precision_var'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../results/test/t-tests.csv\")  \n",
    "\n",
    " \n",
    "id_vars = ['dataset', 'model', 'system', 'ratio']\n",
    "\n",
    "metrics = [\n",
    "    'Full_training_time', 'Accuracy', 'Precision', 'Recall', 'F1_score',\n",
    "    'SPD_gender', 'EOD_gender', 'AOD_gender', 'DI_gender', 'DcI_gender',\n",
    "    'SPD_age', 'EOD_age', 'AOD_age', 'DI_age', 'DcI_age',\n",
    "    'SPD_race', 'EOD_race', 'AOD_race', 'DI_race', 'DcI_race'\n",
    "]\n",
    "\n",
    "suffixes = {\n",
    "    'Mean': '_avg_avg',\n",
    "    'Standard deviation': '_std_avg',\n",
    "    'Variability coefficient': '_vc_avg',\n",
    "    'Selection impact': 'test_{}',\n",
    "    'Variability impact': 'test_{}_var'\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for metric in metrics:\n",
    "        new_row = {key: row[key] for key in id_vars}\n",
    "        new_row[\"Evaluation metric\"] = metric\n",
    "        \n",
    "        for col_name, suffix in suffixes.items():\n",
    "            if metric == \"Full_training_time\" and col_name == \"SelImpact\":\n",
    "                column_name = \"test_time\"\n",
    "            elif '{}' in suffix:\n",
    "                column_name = suffix.format(metric)\n",
    "            else:\n",
    "                column_name = metric + suffix\n",
    "\n",
    "            if column_name in df.columns:\n",
    "                new_row[col_name] = row[column_name]\n",
    "            else:\n",
    "                new_row[col_name] = None\n",
    "\n",
    "        rows.append(new_row)\n",
    "\n",
    "df_reshaped = pd.DataFrame(rows)\n",
    "\n",
    "def should_remove(row):\n",
    "    metric = row[\"Evaluation metric\"]\n",
    "    dataset = row[\"dataset\"].lower()\n",
    "    if dataset == \"ars\" and (metric.endswith(\"_age\") or metric.endswith(\"_race\")):\n",
    "        return True\n",
    "    if dataset.startswith(\"mobiact\") and metric.endswith(\"_race\"):\n",
    "        return True\n",
    "    if dataset.startswith(\"dc\") and metric.endswith(\"_race\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "df_filtered = df_reshaped[~df_reshaped.apply(should_remove, axis=1)]\n",
    "\n",
    "# Sauvegarde dans un nouveau fichier CSV\n",
    "df_filtered.to_csv(\"../../results/test/t-tests-metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AJOUT ROBUSTE DE EC_ID + DROP LIGNES TOUTES VIDES + EXPORT ===\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "IN_METRICS = Path(\"../../results/test/t-tests-metrics.csv\")\n",
    "IN_CFG     = Path(\"../../traces/ExperimentConfigurations.csv\")\n",
    "OUT_STATS  = Path(\"../../traces/ExperimentStatistics.csv\")\n",
    "\n",
    "def s(x):  # strip string\n",
    "    return str(x).strip()\n",
    "\n",
    "def k(x):  # key form\n",
    "    return s(x).lower()\n",
    "\n",
    "# Mappings en minuscules (on mappe sur .lower())\n",
    "MAP_DATASET = {\n",
    "    'census':'Adult','adult':'Adult',\n",
    "    'kdd':'KDD','dc':'DC','mobiact':'MobiAct','ars':'ARS',\n",
    "    'celeba':'CelebA','fairface':'Fairface',\n",
    "    'audiomnist':'audioMNIST','audio mnist':'audioMNIST','audio_mnist':'audioMNIST',\n",
    "    'voxceleb':'voxceleb'\n",
    "}\n",
    "MAP_MODEL = {\n",
    "    'logreg':'LR','lr':'LR','mlp':'MLP','svm':'SVM','dc':'DC',\n",
    "    'audiocnn':'CNN','cnn':'CNN','audiolstm':'LSTM','lstm':'LSTM',\n",
    "    'resnet18':'ResNet18','vgg':'VGG'\n",
    "}\n",
    "MAP_SYSTEM = {\n",
    "    'full':'Full',\n",
    "    'craig':'Craig','craigpb':'Craig',\n",
    "    'glister':'Glister','glisterpb':'Glister',\n",
    "    'gradmatch':'GradMatch','gradmatchpb':'GradMatch',\n",
    "    'random':'Random'\n",
    "}\n",
    "\n",
    "def canon_dataset(v): return MAP_DATASET.get(k(v), s(v))\n",
    "def canon_model(v):   return MAP_MODEL.get(k(v), s(v))\n",
    "def canon_system(v):\n",
    "    kk = re.sub(r'[-_]?pb$', '', k(v))  # retire suffixe PB éventuel\n",
    "    return MAP_SYSTEM.get(kk, s(v))\n",
    "def canon_ratio(x, nd=3):\n",
    "    try: return round(float(x), nd)\n",
    "    except Exception: return None\n",
    "\n",
    "# --- Lecture ---\n",
    "df_res = pd.read_csv(IN_METRICS, dtype=str)\n",
    "df_cfg = pd.read_csv(IN_CFG, dtype=str)\n",
    "\n",
    "need_res = {\"dataset\",\"model\",\"system\",\"ratio\",\"Evaluation metric\"}\n",
    "need_cfg = {\"EC_ID\",\"dataset\",\"model\",\"system\",\"ratio\"}\n",
    "if not need_res.issubset(df_res.columns):\n",
    "    raise ValueError(f\"[t-tests-metrics.csv] colonnes manquantes: {need_res - set(df_res.columns)}\")\n",
    "if not need_cfg.issubset(df_cfg.columns):\n",
    "    raise ValueError(f\"[ExperimentConfigurations.csv] colonnes manquantes: {need_cfg - set(df_cfg.columns)}\")\n",
    "\n",
    "# --- Canonisation des deux côtés ---\n",
    "for df in (df_res, df_cfg):\n",
    "    df[\"dataset_c\"] = df[\"dataset\"].map(canon_dataset)\n",
    "    df[\"model_c\"]   = df[\"model\"].map(canon_model)\n",
    "    df[\"system_c\"]  = df[\"system\"].map(canon_system)\n",
    "    df[\"ratio_c\"]   = df[\"ratio\"].map(lambda x: canon_ratio(x, 3))\n",
    "\n",
    "# Dédup config\n",
    "key = [\"dataset_c\",\"model_c\",\"system_c\",\"ratio_c\"]\n",
    "df_cfg_u = df_cfg.drop_duplicates(key, keep=\"first\")\n",
    "\n",
    "# --- Jointure m:1 pour récupérer EC_ID ---\n",
    "df_j = df_res.merge(df_cfg_u[key + [\"EC_ID\"]], on=key, how=\"left\", validate=\"m:1\")\n",
    "\n",
    "# --- Retirer les lignes où TOUS les 5 champs sont vides ---\n",
    "value_cols = [\"Mean\", \"Standard deviation\", \"Variability coefficient\",\n",
    "              \"Selection impact\", \"Variability impact\"]\n",
    "existing_value_cols = [c for c in value_cols if c in df_j.columns]\n",
    "\n",
    "# convertir chaînes vides/espaces en NA pour ces colonnes\n",
    "for c in existing_value_cols:\n",
    "    df_j[c] = df_j[c].replace(r'^\\s*$', pd.NA, regex=True)\n",
    "\n",
    "if existing_value_cols:\n",
    "    before = len(df_j)\n",
    "    df_j = df_j.dropna(subset=existing_value_cols, how=\"all\")\n",
    "    after = len(df_j)\n",
    "    \n",
    "\n",
    "# --- Retirer la ligne temps comme avant ---\n",
    "df_j = df_j[df_j[\"Evaluation metric\"] != \"Full_training_time\"].copy()\n",
    "\n",
    "# --- Nettoyage des colonnes auxiliaires & ré-ordonnancement ---\n",
    "drop_cols = [\"dataset\",\"model\",\"system\",\"ratio\",\"dataset_c\",\"model_c\",\"system_c\",\"ratio_c\"]\n",
    "df_j.drop(columns=[c for c in drop_cols if c in df_j.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# EC_ID en premier\n",
    "cols = df_j.columns.tolist()\n",
    "if \"EC_ID\" in cols:\n",
    "    cols.insert(0, cols.pop(cols.index(\"EC_ID\")))\n",
    "    df_j = df_j[cols]\n",
    "\n",
    "# --- Export ---\n",
    "df_j.to_csv(OUT_STATS, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille des tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 lignes\n",
      "357 lignes\n",
      "209100 lignes\n",
      "32300 lignes\n",
      "----------------------------------------\n",
      "Total : 241766 lignes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === chemins vers tes fichiers CSV ===\n",
    "file0 = \"../../traces/DatasetProperties.csv\"\n",
    "file1 = \"../../traces/ExperimentConfigurations.csv\"\n",
    "file2 = \"../../traces/ExperimentMeasurements_Adult_KDD.csv\"\n",
    "file3 = \"../../traces/ExperimentMeasurements_ARS.csv\"\n",
    "file4 = \"../../traces/ExperimentMeasurements_DC_MobiAct_celeba_audioMNIST.csv\"\n",
    "file5 = \"../../traces/ExperimentMeasurements_fairface.csv\"\n",
    "file6 = \"../../traces/ExperimentMeasurements_voxceleb.csv\"\n",
    "file7 = \"../../traces/ExperimentStatistics.csv\"\n",
    "\n",
    "# === lecture des fichiers ===\n",
    "df0 = pd.read_csv(file0)\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "df3 = pd.read_csv(file3)\n",
    "\n",
    "# === calcul des tailles ===\n",
    "size0 = len(df0)\n",
    "size1 = len(df1)\n",
    "size2 = len(df2)\n",
    "size3 = len(df3)\n",
    "size4 = len(df3)\n",
    "size5 = len(df3)\n",
    "size6 = len(df3)\n",
    "size7 = len(df3)\n",
    "total = size0 + size1 + size2 + size3 + size4 + size5 + size6 + size7\n",
    "\n",
    "# === affichage ===\n",
    "print(f\"{size0} lignes\")\n",
    "print(f\"{size1} lignes\")\n",
    "print(f\"{size2 + size3 + size4 + size5 + size6} lignes\")\n",
    "print(f\"{size7} lignes\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total : {total} lignes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leace-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
